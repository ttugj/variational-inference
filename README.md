# Synopsis 

This library implements an approach to variational inference loosely
following [ADVI](https://arxiv.org/abs/1603.00788) by Kucukelbir et al.
It is a work in progress, aimed at testing a particular design.

# Background

## Variational inference

Consider the typical computational problem encountered in probabilistic inference:
given a probability distribution μ over some domain, known only via its density
(with respect to some reference measure) up to a (computationally untractable)
normalisation constant, compute statistics such as the expected value of a
given function over the domain.

The distribution may arise from the joint distribution of some probabilistic
model conditioned on observed data, as in Bayesian inference, but other
scenarios are possible as well. One approach to the problem is to sample _directly_
from μ using MCMC methods (expected values are then estimated by averaging over
samples). Another is to _approximate_ μ by a more easily controlled probability distribution,
described by immediately interpretable parameters and straightforward to sample from.

In the latter case, one fixes a _variational family_ consisting of distributions ν(θ) parameterised
by some finite-dimensional parameter domain Θ. Mapping θ ∈ Θ to the Kullback-Leibler divergence
between ν(θ) and μ defines a loss function _L_(θ) = _D_(ν(θ)|μ) on Θ,
and ν(argmin _L_) is the resulting variational approximation. Thus the problem is reduced to 
minimising loss, a pure optimisation task.

## ADVI

Variational inference becomes difficult for larger, hierarchical models: given several levels
of priors, composing distributions with constrained parameters (as in the usual parameterisations
of Beta or Gamma), both constructing a variational family that can reasonably well approximate
the target distribution, as well as minimising loss over the variational parameter space, are non-trivial 
to perform and debug. The ADVI recipe is, in a simplified form, to:

1. reparameterise to remove constraints (e.g. apply _log_ to positive parameters, _logit_ to parameters in [0,1], etc.);
2. use a single multivariate normal variational family over the entire unconstrained parameter space (full covariance, or mean field);
3. use the reparameterisation trick and leverage automatic differentiation to sample estimates of loss gradient (parameterising
variational distributions by mean and a Cholesky factor of covariance).

# Design

## Mathematical model

### Quasiarrows

Given a sufficiently rich category of spaces __C__, it is most convenient to formalise probability theory
in terms of a monad _P_ : __C__ → __C__, with _PX_ interpreted as the space of probability distributions on _X_.
This is too much to hope for in our context.

First, since we need automatic differentiation, we'll work over a
much more restricted category of _domains_ (in a first approximation equivalent to the category of Cartesian
spaces and smooth maps). We thus lose representability, weakening _P_ to a profunctor _p_ : __C°__ × __C__ → __Set__,
with _p_(_X_,_Y_) interpreted as the set of families of probability distributions over _Y_ parameterised by _X_.
In fact, our category will be Cartesian, and the profunctor _p_ would then be an _arrow_ (in the usual Haskell sense),
a shadow of _P_ being a monad.

However, since marginals are in general intractable, we do not even have functoriality in the second argument of _p_! 
Likewise, there are no composition maps _p_(_X_,_Y_) × _p_(_Y_,_Z_) → _p_(_X_,_Z_). Instead, _p_(_X_,-) is functorial only
with respect to canonical isomorphisms induced by the Cartesian structure (generated by symmetry and associativity of the
product), while composition is replaced by general mixture maps 
_p_(_X_,_Y_) × _p_(_X_ × _Y_, _Z_) → _p_(_X_, _Y_ × _Z_). The latter allow us to construct _joint_ distributions over all intermediate
variables instead of a marginal distribution over some final subset thereof. Finally, since we want to work with probability densities,
we _do not_ assume the existence of lift maps __C__(_X_,_Y_) → _p_(_X_,_Y_)
that would arise as a shadow of the monadic unit.

These properties give rise to what we'll call a _quasiarrow_. Informally, a quasiarrow on a Cartesian category __C__ is:[^1]

* for each _X_,_Y_ ∈ Ob __C__, a set _p_(_X_,_Y_);
* for each _Y_ ∈ Ob __C__, the structure of a presheaf _p_(-,_Y_) ∈ PSh __C__;
* for each _X_ ∈ Ob __C__, the structure of a set-valued functor _p_(_X_,-) from the subgroupoid of __C__ generated by symmetry and associativity of the Cartesian product;
* for each _X_,_Y_,_Z_ ∈ Ob __C__, a general mixture map[^2] 
_p_(_X_,_Y_) × _p_(_X_ × _Y_, _Z_) → _p_(_X_, _Y_ × _Z_); 

satisfying natural compatibility conditions that would arise if _p_ were of the form Hom ∘ (id × _P_) for a monad _P_ on __C__. 

[^2]: Note that using the general mixture map and pullbacks along the two projections from _X_ × _Y_ we may form
a simple mixture map
_p_(_X_,_Y_) × _p_(_Y_,_Z_) → _p_(_X_, _Y_ × _Z_),
as well as the 'tensor product'
_p_(_X_,_Y_) × _p_(_X_,_Z_) → _p_(_X_, _Y_ × _Z_).

Now, modelling probability in terms of a quasiarrow on a suitable category of domains allows us to combine probabilistic and differentiable programming.
In particular:

* the existence of mixture maps, being a shadow of the monadic multiplication _P²_ → _P_, encodes the 'algebraic' aspect of probability,
* the space of probability distributions on a domain is 'represented' by a presheaf over domains, thus inheriting their 'differentiable' aspect.

[^1]: More formally, let __C__ be a Cartesian category,
and let Braid __C__ → __C__ be the core groupoid of the free Cartesian category on Ob __C__,
together with the Cartesian embedding acting as identity on Ob __C__ ⊂ Ob Braid __C__.

    A _quasiarrow_ on __C__ is a map _p_ : __C°__ × __C__ → __Set__ together with:

    1. pullback maps __C__(_X'_,_X_) × _p_(_X_,_Y_) → _p_(_X'_,_Y_) extending _p_(-,_Y_) to a presheaf on __C__,
    2. general mixture maps _p_(_X_,_Y_) × _p_(_X_ × _Y_, _Z_) → _p_(_X_, _Y_ × _Z_),
    3. pushforward maps Braid __C__(_Y_,_Y'_) × _p_(_X_,_Y_) → _p_(_X_,_Y_') extending the restriction of _p_(_X_,-) to
    a set-valued functor on Braid __C__;

    satisfying natural compatibility conditions: pullbacks commute with pushforwards, mixtures are compatible with both pullbacks
    and pushforwards, and furthermore associative in the sense that the two composites
    mix ∘ (mix × id), mix ∘ (id × mix): _p_(_X_, _Y_) × _p_(_X_ × _Y_, _Z_) × _p_(_X_ × _Y_ × _Z_, _W_) → _p_(_X_, _Y_ × _Z_ × _W_)
    coincide (where we implicitly use functoriality of _p_(_X_, -) with respect to Braid __C__ to disambiguate iterated products).

### Jets and domains

The most straightforward category to describe parameter spaces of differentiable models would be the Lawvere
theory[^3] __CartSp__ of Cartesian spaces,
with the naturals ℕ as objects and __CartSp__(_m_, _n_) = _C_^∞(ℝ^m, ℝ^n) as morphisms.

[^3]: We use the following definition. Let __Fin__ be the category whose objects are the naturals ℕ, and
whose morphisms _m_ → _n_ are maps {1,‥,m} → {1,‥,n}. Note that the opposite category __Fin°__ is naturally Cartesian.
Then a Lawvere theory is a small Cartesian category __T__ with ℕ as its object set,
together with a Cartesian functor __Fin°__ → __T__ acting as identity on objects.

A more accurate model of how differentiability is actually implemented is another Lawvere theory
we'll denote by __J__. Its objects are necessarily still ℕ, and __J__(_m_,_n_) is geometrically the set 
of set-theoretic sections of the first jet bundle of the trivial ℝ^n-fibration over ℝ^m. 
In simple terms, a morphism
_m_ → _n_ in __J__ sends each point _x_ ∈ ℝ^m to a point _y_ ∈ ℝ^n together with a linear map ℝ^m → ℝ^n
of corresponding tangent spaces.

There is a natural homomorphism of Lawvere theories __CartSp__ → __J__, whose action on morphisms
sends a _C_^∞ function ℝ^m → ℝ^n to its first jet. In simple terms, _f_ : ℝ^m → ℝ^n is sent
to the morphism in __J__ sending _x_ ∈ ℝ^m to _f_(_x_) together with the Jacobian _df_(_x_) : ℝ^m → ℝ^n.  

Thus, __J__ encodes information about gradients, without modelling full differentiability: this will
be enough for first-order differentiable programming, in particular for typical SGD algorithms.

Finally, the category of domains we'll use may be viewed as equivalent to __J__, so that in particular
it admits a full and faithful functor into __J__. Its objects are meant to represent, in addition to
the ℝ^n, domains such as positive orthants in the ℝ^n, cubes, simplices, spaces of positive-definite matrices
etc.
-- each equipped with some distinguished diffeomorphism to an ℝ^n. This allows us to model standard probabilistic
parameter spaces as nominally distinct objects, while internally computing with plain unconstrained ℝ^n both as
spaces of transformed parameters, and as target spaces for probability densities (for example,
the `Density` quasiarrow on the category of domains sends a pair of domains _X_, _Y_ into
__J__(dim _X_ + dim _Y_, 1) as the space of parameterised log-densities).

## Implementation 

### `VI.Categories`
### `VI.Jets`
### `VI.Domains`
### `VI.Quasiarrows`
### `VI.Inference`

