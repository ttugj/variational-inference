# Synopsis 

This library implements an approach to variational inference loosely
following [ADVI](https://arxiv.org/abs/1603.00788) by Kucukelbir _et al._
It is a work in progress, aimed at testing a particular design.

# Background

## Variational inference

Consider the typical computational problem of probabilistic inference:
given a probability distribution μ over some domain, known only via its density
(with respect to some reference measure) up to a (computationally untractable)
normalisation constant, compute statistics such as the expected value of a
given function over the domain.

The distribution may arise from the joint distribution of some probabilistic
model conditioned on observed data, as in Bayesian inference, but other
scenarios are possible as well. One approach to the problem is to sample _directly_
from μ using MCMC methods (expected values are then estimated by averaging over
samples). Another is to _approximate_ μ by a more easily controlled probability distribution,
described by immediately interpretable parameters and straightforward to sample from.

In the latter case, one fixes a _variational family_ consisting of distributions ν(θ) parameterised
by some finite-dimensional parameter domain Θ. Mapping θ ∈ Θ to the Kullback-Leibler divergence
between ν(θ) and μ defines a loss function _L_(θ) = _D_(ν(θ)|μ) on Θ,
and ν(argmin _L_) is the resulting variational approximation. Thus the problem is reduced to 
minimising loss, a pure optimisation task.

## ADVI

Variational inference becomes difficult for larger, hierarchical models: given several levels
of priors, composing distributions with constrained parameters (as in the usual parameterisations
of Beta or Gamma), both constructing a variational family that can reasonably well approximate
the target distribution, as well as minimising loss over the variational parameter space, are non-trivial 
to perform and debug. The ADVI recipe is, in a simplified form, to:

1. reparameterise to remove constraints (e.g. apply _log_ to positive parameters, _logit_ to parameters in [0,1], etc.);
2. use a single multivariate normal variational family over the entire unconstrained parameter space (full covariance, or mean field);
3. use the reparameterisation trick and leverage automatic differentiation to sample estimates of loss gradient (parameterising
variational distributions by mean and a Cholesky factor of covariance).

# Design

## Mathematical model

Given a sufficiently rich category of spaces __C__, it is most convenient to formalise probability theory
in terms of a monad _P_ : __C__ → __C__, with _PX_ interpreted as the space of probability distributions on _X_.
This is too much to hope for in our context.

First, since we need automatic differentiation, we'll work over a
much more restricted category of _domains_ (in a first approximation equivalent to the category of Cartesian
spaces and smooth maps). We thus lose representability, weakening _P_ to a profunctor _p_:__C__^op × __C__ → __Set__,
with _p_(_X_,_Y_) interpreted as the set of families of probability distributions over _Y_ parameterised by _X_.
In fact, our category will be Cartesian, and the profunctor _p_ would then be an _arrow_ (in the usual Haskell sense),
a shadow of _P_ being a monad.

However, since marginals are in general intractable, we do not even have functoriality in the second argument of _p_!
Likewise, there are no composition maps _p_(_X_,_Y_) × _p_(_Y_,_Z_) → _p_(_X_,_Z_). These are replaced by general mixture maps 
_p_(_X_,_Y_) × _p_(_X_ × _Y_, _Z_) → _p_(_X_, _Y_ × _Z_), allowing us to construct _joint_ distributions over all intermediate
variables instead of a marginal distribution over some final subset thereof. We are also able to restore functoriality
in the second argument _restricted_ to _canonical isomorphisms_ defined by the Cartesian structure alone (generated by
symmetry and associativity of the Cartesian product). 

These properties give rise to what we'll call a _quasiarrow_.

## Implementation 

### `VI.Categories`
### `VI.Jets`
### `VI.Domains`
### `VI.Quasiarrows`
### `VI.Inference`

